{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "import os\n",
    "\n",
    "# åœ¨ä»£ç å¼€å¤´è®¾ç½®ç¯å¢ƒå˜é‡\n",
    "os.environ[\"ARK_API_KEY\"] = \"611916e2-f8b6-47da-aa76-1b635ec01f3e\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "import pandas as pd\n",
    "import re\n",
    "import sys\n",
    "import subprocess\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.documents import Document\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "\n",
    "# é…ç½®å‚æ•°\n",
    "PDF_PATH = \"data/æœˆå­ä¸­å¿ƒäº§å“çŸ¥è¯†åº“é›å½¢è®­ç»ƒ.pdf\"\n",
    "EXCEL_PATH = \"data/ä¸Šæµ·æœˆå­ä¸­å¿ƒè¡¨æ ¼.xlsx\"\n",
    "\n",
    "# å®šä¹‰æ–‡æ¡£é“¾æ¥æ˜ å°„ï¼ˆéœ€æ ¹æ®å®é™…ä¿®æ”¹ï¼‰\n",
    "DOCUMENT_LINKS = {\n",
    "    \"æœˆå­ä¸­å¿ƒäº§å“çŸ¥è¯†åº“\": \"https://example.com/product_knowledge\",\n",
    "    \"ä¸Šæµ·æœˆå­ä¸­å¿ƒæ•°æ®åº“\": \"https://example.com/shanghai_center\",\n",
    "    \"æ¯å©´æŠ¤ç†æŒ‡å—\": \"https://example.com/maternal_care\"\n",
    "}\n",
    "\n",
    "# 1. æ–‡æ¡£åŠ è½½ä¸é¢„å¤„ç†\n",
    "def load_and_preprocess_data():\n",
    "    documents = []\n",
    "    \n",
    "    # åŠ è½½PDFæ–‡æ¡£ - ä½¿ç”¨PyPDFLoaderè‡ªåŠ¨å¤„ç†ä¸ºDocumentå¯¹è±¡\n",
    "    try:\n",
    "        pdf_loader = PyPDFLoader(PDF_PATH)\n",
    "        pdf_pages = pdf_loader.load()\n",
    "        for page in pdf_pages:\n",
    "            # ç¡®ä¿metadataå­˜åœ¨\n",
    "            if not hasattr(page, 'metadata') or not isinstance(page.metadata, dict):\n",
    "                page.metadata = {}\n",
    "            page.metadata[\"source\"] = \"æœˆå­ä¸­å¿ƒäº§å“çŸ¥è¯†åº“\"\n",
    "            documents.append(page)\n",
    "        print(f\"âœ… PDFæ–‡æ¡£åŠ è½½å®Œæˆ: {len(pdf_pages)}é¡µ\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ PDFåŠ è½½é”™è¯¯: {str(e)}\")\n",
    "    \n",
    "    # åŠ è½½Excelæ•°æ® - ç¡®ä¿åˆ›å»ºä¸ºDocumentå¯¹è±¡\n",
    "    try:\n",
    "        df = pd.read_excel(EXCEL_PATH)\n",
    "        excel_count = 0\n",
    "        \n",
    "        # Excelåˆ—åå¤„ç†ï¼ˆç¡®ä¿åˆ—åæ­£ç¡®ï¼‰\n",
    "        column_mapping = {\n",
    "            \"å“ç‰Œåç§°\": \"å“ç‰Œåç§°\",\n",
    "            \"åˆ†åº—åç§°\": \"åˆ†åº—åç§°\",\n",
    "            \"åŒºåŸŸ\": \"åŒºåŸŸ\",\n",
    "            \"ç‰¹è‰²æœåŠ¡\": \"ç‰¹è‰²æœåŠ¡\",\n",
    "            \"æŠ¤ç†æ¨¡å¼\": \"æŠ¤ç†æ¨¡å¼\",\n",
    "            \"åŒ»ç–—èµ„æº\": \"åŒ»ç–—èµ„æº\",\n",
    "            \"é¤é¥®ç‰¹è‰²\": \"é¤é¥®ç‰¹è‰²\",\n",
    "            \"ç¯å¢ƒç‰¹è‰²åŠæˆ¿å‹é£æ ¼+å‘¨è¾¹\": \"ç¯å¢ƒç‰¹è‰²\",\n",
    "            \"ç¡¬ä»¶è®¾æ–½\": \"ç¡¬ä»¶è®¾æ–½\",\n",
    "            \"å¼€åº—å¹´ä»½\": \"å¼€ä¸šå¹´ä»½\",\n",
    "            \"äº§åº·\": \"äº§åº·æœåŠ¡\",\n",
    "            \"åœ°å€\": \"åœ°å€\",\n",
    "            \"ç±»å‹\": \"ç±»å‹\",\n",
    "            \"ä»·æ ¼èŒƒå›´\": \"ä»·æ ¼èŒƒå›´\",\n",
    "            \"æˆ¿å‹ä¿¡æ¯\": \"æˆ¿å‹ä¿¡æ¯\",\n",
    "            \"å®¢æˆ·è¯„ä»·ç‰¹ç‚¹\": \"å®¢æˆ·è¯„ä»·\"\n",
    "        }\n",
    "        \n",
    "        # ä¿®æ­£åˆ—åæ˜ å°„\n",
    "        available_columns = df.columns.tolist()\n",
    "        actual_mapping = {}\n",
    "        for target, source in column_mapping.items():\n",
    "            if source in available_columns:\n",
    "                actual_mapping[source] = target\n",
    "            elif target in available_columns:\n",
    "                actual_mapping[target] = target\n",
    "        \n",
    "        # åˆ›å»ºæ–‡æ¡£\n",
    "        for _, row in df.iterrows():\n",
    "            doc_content_parts = []\n",
    "            for col, alias in actual_mapping.items():\n",
    "                value = row.get(col, \"\")\n",
    "                if pd.notna(value):\n",
    "                    doc_content_parts.append(f\"{alias}: {value}\")\n",
    "            \n",
    "            if doc_content_parts:\n",
    "                doc_content = \"\\n\".join(doc_content_parts)\n",
    "                # åˆ›å»ºä¸ºæ ‡å‡†çš„Documentå¯¹è±¡\n",
    "                doc = Document(\n",
    "                    page_content=doc_content,\n",
    "                    metadata={\"source\": \"ä¸Šæµ·æœˆå­ä¸­å¿ƒæ•°æ®åº“\"}\n",
    "                )\n",
    "                documents.append(doc)\n",
    "                excel_count += 1\n",
    "        \n",
    "        print(f\"âœ… Excelæ•°æ®åŠ è½½å®Œæˆ: {excel_count}æ¡è®°å½•\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ ExcelåŠ è½½é”™è¯¯: {str(e)}\")\n",
    "        # æ‰“å°è¯¦ç»†é”™è¯¯ä¿¡æ¯\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    return documents\n",
    "\n",
    "# 2. æ–‡æœ¬åˆ†å‰² - ä½¿ç”¨æ”¹è¿›çš„åˆ†å‰²å™¨\n",
    "def split_documents(docs):\n",
    "    # åˆ›å»ºè‡ªå®šä¹‰åˆ†å‰²å™¨ï¼Œå¤„ç†å¤šç§æ–‡æ¡£æ ¼å¼\n",
    "    class CustomTextSplitter(RecursiveCharacterTextSplitter):\n",
    "        def split_documents(self, documents):\n",
    "            results = []\n",
    "            for doc in documents:\n",
    "                # å¤„ç†ä¸åŒæ ¼å¼çš„æ–‡æ¡£\n",
    "                if isinstance(doc, Document):\n",
    "                    text = doc.page_content\n",
    "                    metadata = doc.metadata\n",
    "                elif isinstance(doc, dict) and \"page_content\" in doc:\n",
    "                    text = doc[\"page_content\"]\n",
    "                    metadata = doc.get(\"metadata\", {})\n",
    "                elif isinstance(doc, str):\n",
    "                    text = doc\n",
    "                    metadata = {}\n",
    "                else:\n",
    "                    continue\n",
    "                \n",
    "                # åˆ†å‰²æ–‡æœ¬\n",
    "                chunks = self.split_text(text)\n",
    "                \n",
    "                # åˆ›å»ºDocumentå¯¹è±¡\n",
    "                for chunk in chunks:\n",
    "                    new_doc = Document(page_content=chunk, metadata=metadata.copy())\n",
    "                    results.append(new_doc)\n",
    "            \n",
    "            return results\n",
    "    \n",
    "    text_splitter = CustomTextSplitter(\n",
    "        chunk_size=400,\n",
    "        chunk_overlap=50,\n",
    "        length_function=len\n",
    "    )\n",
    "    return text_splitter.split_documents(docs)\n",
    "\n",
    "# 3. å‘é‡å­˜å‚¨\n",
    "def create_vector_store(splits):\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-large-zh-v1.5\")\n",
    "    return FAISS.from_documents(splits, embeddings)\n",
    "\n",
    "# 4. æ™ºèƒ½å›ç­”ç”Ÿæˆå™¨\n",
    "class MaternityAdvisor:\n",
    "    def __init__(self, vector_store):\n",
    "        self.vector_store = vector_store\n",
    "        self.retriever = vector_store.as_retriever(search_kwargs={\"k\": 6})\n",
    "        \n",
    "       \n",
    "        self.client = OpenAI(\n",
    "            base_url=\"https://ark.cn-beijing.volces.com/api/v3\",\n",
    "            api_key=os.environ.get(\"ARK_API_KEY\")\n",
    "        )\n",
    "        \n",
    "        # è±†åŒ…æ¨¡å‹é…ç½®\n",
    "        self.model_name = \"doubao-1-5-thinking-pro-250415\"\n",
    "    \n",
    "    def _should_reject(self, question):\n",
    "        \"\"\"åˆ¤æ–­æ˜¯å¦åº”è¯¥æ‹’ç»å›ç­”é—®é¢˜\"\"\"\n",
    "        if not question.strip():\n",
    "            return \"ç©ºç™½é—®é¢˜\"\n",
    "            \n",
    "        # éæ¯å©´é¢†åŸŸå…³é”®è¯æ£€æµ‹\n",
    "        non_domain_keywords = [\"æ”¿æ²»\", \"èµŒåš\", \"è‰²æƒ…\", \"æš´åŠ›\", \"é‡‘è\", \"è‚¡ç¥¨\", \"ç¼–ç¨‹\", \"æŠ€æœ¯\"]\n",
    "        if any(keyword in question for keyword in non_domain_keywords):\n",
    "            return \"éé¢†åŸŸé—®é¢˜\"\n",
    "            \n",
    "        # ä¸ªäººä¿¡æ¯æ£€æµ‹\n",
    "        personal_patterns = [r\"ç”µè¯\", r\"æ‰‹æœºå·\", r\"èº«ä»½è¯\", r\"ä½å€\", r\"å¯†ç \"]\n",
    "        if any(re.search(pattern, question, re.IGNORECASE) for pattern in personal_patterns):\n",
    "            return \"æ¶‰åŠéšç§ä¿¡æ¯\"\n",
    "            \n",
    "        # äººå·¥å®¢æœè§¦å‘è¯\n",
    "        if \"äººå·¥\" in question:\n",
    "            return \"äººå·¥å®¢æœè¯·æ±‚\"\n",
    "            \n",
    "        return False\n",
    "    \n",
    "    def _add_caring_notes(self, response, question):\n",
    "        \"\"\"æ·»åŠ å…³æ€€æ€§è”æƒ³å»ºè®®\"\"\"\n",
    "        # é¢„äº§æœŸå­£èŠ‚è”æƒ³\n",
    "        if \"é¢„äº§æœŸ\" in question:\n",
    "            month_match = re.search(r\"(\\d{1,2})æœˆ\", question)\n",
    "            if month_match:\n",
    "                month = int(month_match.group(1))\n",
    "                if month in [12, 1, 2]:\n",
    "                    response += \"\\n\\nâ„ï¸ å†¬å­£å°è´´å£«ï¼šå»ºè®®é€‰æ‹©æœ‰åœ°æš–å’Œç‹¬ç«‹ç©ºè°ƒçš„æœˆå­æˆ¿å‹ï¼Œæ³¨æ„å®å®çš„ä¿æš–æŠ¤ç†\"\n",
    "                elif month in [6, 7, 8]:\n",
    "                    response += \"\\n\\nâ˜€ï¸ å¤å­£å°è´´å£«ï¼šå»ºè®®é€‰æ‹©é€šé£è‰¯å¥½ã€æœ‰ç©ºè°ƒçš„æˆ¿å‹ï¼Œæ³¨æ„é˜²æš‘å’Œå®å®çš®è‚¤æŠ¤ç†\"\n",
    "            else:\n",
    "                response += \"\\n\\nğŸŒ¸ æ¸©é¦¨æé†’ï¼šä¸åŒå­£èŠ‚æœ‰ä¸åŒçš„æ¯å©´æŠ¤ç†é‡ç‚¹å‘¢~\"\n",
    "        \n",
    "        # æ–°ç”Ÿå„¿æŠ¤ç†è”æƒ³\n",
    "        if \"æ–°ç”Ÿå„¿\" in question or \"å®å®\" in question:\n",
    "            if \"å–‚å…»\" in question:\n",
    "                response += \"\\n\\nğŸ¼ å–‚å…»å»ºè®®ï¼šæ¯ä¹³å–‚å…»å¯¹å®å®å…ç–«åŠ›å¾ˆé‡è¦å“¦\"\n",
    "            if \"ç¡çœ \" in question:\n",
    "                response += \"\\n\\nğŸ˜´ ç¡çœ æç¤ºï¼šæ–°ç”Ÿå„¿éœ€è¦å……è¶³ç¡çœ æ‰èƒ½å¥åº·æˆé•¿\"\n",
    "            if \"æ´—æ¾¡\" in question:\n",
    "                response += \"\\n\\nğŸ›€ æ´—æ¾¡å°è´´å£«ï¼šæ°´æ¸©ä¿æŒ37â„ƒå·¦å³æœ€èˆ’é€‚\"\n",
    "        \n",
    "        # äº§åæ¢å¤è”æƒ³\n",
    "        if \"äº§å\" in question or \"å¦ˆå¦ˆ\" in question:\n",
    "            response += \"\\n\\nğŸ’• å¦ˆå¦ˆå…³æ€€ï¼šäº§åæ¢å¤æœŸé—´ä¹Ÿè¦å…³æ³¨è‡ªå·±çš„èº«å¿ƒå¥åº·\"\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    # def _add_document_links(self, context_docs):\n",
    "    #     \"\"\"æ·»åŠ ç›¸å…³æ–‡æ¡£é“¾æ¥\"\"\"\n",
    "    #     sources = set()\n",
    "    #     for doc in context_docs:\n",
    "    #         if hasattr(doc, 'metadata') and isinstance(doc.metadata, dict) and \"source\" in doc.metadata:\n",
    "    #             sources.add(doc.metadata[\"source\"])\n",
    "        \n",
    "    #     if not sources:\n",
    "    #         return \"\"\n",
    "            \n",
    "    #     links_section = \"\\n\\n**ç›¸å…³å‚è€ƒæ–‡æ¡£ï¼š**\\n\"\n",
    "    #     for source in sources:\n",
    "    #         if source in DOCUMENT_LINKS:\n",
    "    #             links_section += f\"- [{source}]({DOCUMENT_LINKS[source]})\\n\"\n",
    "        \n",
    "    #     return links_section\n",
    "    \n",
    "    def generate_response(self, question):\n",
    "        \"\"\"ç”Ÿæˆå›ç­”\"\"\"\n",
    "        # 0. å¤„ç†ç©ºç™½é—®é¢˜\n",
    "        if not question.strip():\n",
    "            return \"ğŸ‘¶ äº²çˆ±çš„ï¼Œæ˜¯ä¸æ˜¯å¿˜è®°è¾“å…¥é—®é¢˜äº†å‘¢ï¼Ÿæˆ‘å¾ˆä¹æ„å¸®æ‚¨è§£ç­”æ¯å©´ç›¸å…³é—®é¢˜å“¦~\"\n",
    "        \n",
    "        # 1. æ£€æŸ¥æ˜¯å¦åº”è¯¥æ‹’ç»å›ç­”\n",
    "        reject_reason = self._should_reject(question)\n",
    "        if reject_reason:\n",
    "            if reject_reason == \"äººå·¥å®¢æœè¯·æ±‚\":\n",
    "                return \"ğŸ‘¶ å¥½å‘¦ï¼Œæ²¡èƒ½è§£ç­”æ‚¨çš„ç–‘é—®å‘¢~è¯·ç‚¹å‡»[äººå·¥å®¢æœ](https://example.com/customer_service)å¯»æ±‚å¸®åŠ©ï¼Œæˆ‘ä»¬ä¼šå°½å¿«ä¸ºæ‚¨æœåŠ¡ï¼\"\n",
    "            elif reject_reason == \"æ¶‰åŠéšç§ä¿¡æ¯\":\n",
    "                return \"ğŸ™ ä¸å¥½æ„æ€å•¦ï¼Œä¸ºäº†ä¿æŠ¤æ‚¨çš„éšç§å®‰å…¨ï¼Œæˆ‘ä¸èƒ½å¤„ç†æ­¤ç±»ä¿¡æ¯å‘¢~\"\n",
    "            else:\n",
    "                return \"ğŸ™ æŠ±æ­‰å‘¦ï¼Œè¿™ä¸ªé—®é¢˜è¶…å‡ºäº†æ¯å©´é¢†åŸŸçš„èŒƒå›´å‘¢ã€‚å¦‚æœæ˜¯å…³äºå­•æœŸã€æ–°ç”Ÿå„¿æŠ¤ç†æˆ–æœˆå­ä¸­å¿ƒçš„é—®é¢˜ï¼Œæˆ‘å¾ˆä¹æ„ä¸ºæ‚¨è§£ç­”~\"\n",
    "        \n",
    "        # 2. æ£€ç´¢ç›¸å…³æ–‡æ¡£\n",
    "        try:\n",
    "            context_docs = self.retriever.get_relevant_documents(question)\n",
    "        except Exception as e:\n",
    "            print(f\"æ£€ç´¢é”™è¯¯: {str(e)}\")\n",
    "            context_docs = []\n",
    "            \n",
    "        if not context_docs:\n",
    "            return \"ğŸ˜” å¯¹ä¸èµ·ï¼Œæˆ‘å·²ç»å­¦ä¹ çš„çŸ¥è¯†ä¸­ä¸åŒ…å«é—®é¢˜ç›¸å…³å†…å®¹ï¼Œæš‚æ—¶æ— æ³•æä¾›ç­”æ¡ˆã€‚å¦‚æœæ‚¨æœ‰æ¯å©´ã€å©šè‚²é¢†åŸŸç›¸å…³çš„å…¶ä»–é—®é¢˜ï¼Œæˆ‘ä¼šå°è¯•å¸®åŠ©æ‚¨è§£ç­”~\"\n",
    "        \n",
    "        # 3. æå–ä¸Šä¸‹æ–‡æ–‡æœ¬\n",
    "        context_texts = []\n",
    "        for doc in context_docs:\n",
    "            if isinstance(doc, Document):\n",
    "                context_texts.append(doc.page_content)\n",
    "            elif isinstance(doc, dict) and \"page_content\" in doc:\n",
    "                context_texts.append(doc[\"page_content\"])\n",
    "            elif isinstance(doc, str):\n",
    "                context_texts.append(doc)\n",
    "        \n",
    "        if not context_texts:\n",
    "            return \"ğŸ˜” æœªèƒ½è·å–åˆ°ç›¸å…³æ•°æ®å‘¢ï¼Œè¯·å°è¯•å…¶ä»–æé—®æ–¹å¼~\"\n",
    "            \n",
    "        context_text = \"\\n\\n\".join(context_texts)\n",
    "        prompt=\"\"\"\n",
    "            <ç³»ç»Ÿè®¾å®š>\n",
    "            ä½ æ˜¯ä¸“ä¸šæ¯å©´å©šè‚²é¡¾é—®å°è´ï¼Œæä¾›ç²¾å‡†ã€è´´å¿ƒçš„å’¨è¯¢æœåŠ¡ã€‚\n",
    "            \n",
    "            <è¦æ±‚>\n",
    "            1. ä½¿ç”¨æ¸©æŸ”è¯­æ°”è¯å¦‚\"å‘¢\"ã€\"å•¦\"ã€\"å¥½å‘¦\"\n",
    "            2. ç»“åˆä¸Šä¸‹æ–‡æä¾›å®ç”¨å»ºè®®\n",
    "            3. ç®€æ´æ˜äº†ï¼Œä¸è¶…è¿‡400å­—\n",
    "            4. ä½¿ç”¨Markdownæ ¼å¼\n",
    "            5. æœ€ååˆ—å‡ºç›¸å…³æ–‡æ¡£é“¾æ¥\n",
    "            6. å›ç­”ç»“æŸåï¼Œä¸»åŠ¨æå‡º3ä¸ªç”¨æˆ·å¯èƒ½å…³å¿ƒçš„ç›¸å…³é—®é¢˜\n",
    "                - è¿™äº›é—®é¢˜å¿…é¡»åŸºäºå½“å‰å›ç­”å†…å®¹\n",
    "                - ç¡®ä¿é—®é¢˜åœ¨çŸ¥è¯†åº“ä¸­æœ‰ç­”æ¡ˆ\n",
    "                - æ ¼å¼ï¼šä»¥\"æ‚¨å¯èƒ½è¿˜æƒ³äº†è§£ï¼š\"å¼€å¤´ï¼Œæ¯ä¸ªé—®é¢˜ç¼–å·åˆ—å‡º\n",
    "            \n",
    "            <å›ç­”æŒ‡å—>\n",
    "            - ä»…åŸºäºä¸Šä¸‹æ–‡å›ç­”\n",
    "            - è¯­è¨€æ¸©æš–ä¸“ä¸š\n",
    "            - æœ€åå•ç‹¬ä¸€è¡Œé™„æ–‡æ¡£é“¾æ¥\n",
    "            - é¿å…ç¼–é€ ä¿¡æ¯\n",
    "            - æ¨èçš„é—®é¢˜å¿…é¡»èƒ½åœ¨çŸ¥è¯†åº“ä¸­æ‰¾åˆ°ç­”æ¡ˆ\"\"\"\n",
    "        # 4. ç”Ÿæˆå›ç­”\n",
    "        try:\n",
    "        # æ„å»ºè±†åŒ…APIéœ€è¦çš„messagesæ ¼å¼\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": prompt},\n",
    "                {\"role\": \"user\", \"content\": context_text }\n",
    "            ]\n",
    "            \n",
    "            # è°ƒç”¨è±†åŒ…API\n",
    "            completion = self.client.chat.completions.create(\n",
    "                model=self.model_name,\n",
    "                messages=messages,\n",
    "                temperature=0.2,\n",
    "                max_tokens=1000\n",
    "            )\n",
    "            \n",
    "            response = completion.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            print(f\"è±†åŒ…APIé”™è¯¯: {str(e)}\")\n",
    "            response = \"ğŸ”„ å“å‘€ï¼Œå›ç­”é—®é¢˜æ—¶å‡ºäº†ç‚¹å°é—®é¢˜~æ‚¨å¯ä»¥æ¢ä¸ªæ–¹å¼æé—®æˆ–è€…ç¨åå†è¯•å—ï¼Ÿ\"\n",
    "        \n",
    "        # 5. åå¤„ç†\n",
    "        try:\n",
    "            # æ·»åŠ æ¸©é¦¨å…³æ€€å»ºè®®\n",
    "            response = self._add_caring_notes(response, question)\n",
    "            \n",
    "            # # æ·»åŠ æ–‡æ¡£é“¾æ¥\n",
    "            # doc_links = self._add_document_links(context_docs)\n",
    "            # response += doc_links\n",
    "        except Exception as e:\n",
    "            print(f\"åå¤„ç†é”™è¯¯: {str(e)}\")\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def _limit_length(self, response):\n",
    "        \"\"\"ç¡®ä¿å›ç­”ä¸è¶…è¿‡å­—æ•°é™åˆ¶\"\"\"\n",
    "        if len(response) > 350:\n",
    "            # ä¿ç•™å¼€å¤´å’Œç»“å°¾\n",
    "            start = response[:200]\n",
    "            end = \"\\n\\nğŸ“„ï¼ˆå†…å®¹å·²ç²¾ç®€ï¼Œè¯·å‚è€ƒç›¸å…³æ–‡æ¡£è·å–æ›´å¤šä¿¡æ¯ï¼‰\" + self._extract_doc_links(response)\n",
    "            return start + end\n",
    "        return response\n",
    "    \n",
    "    def _extract_doc_links(self, response):\n",
    "        \"\"\"ä»åŸå“åº”ä¸­æå–æ–‡æ¡£é“¾æ¥éƒ¨åˆ†\"\"\"\n",
    "        if \"ç›¸å…³å‚è€ƒæ–‡æ¡£ï¼š\" in response:\n",
    "            parts = response.split(\"ç›¸å…³å‚è€ƒæ–‡æ¡£ï¼š\")\n",
    "            if len(parts) > 1:\n",
    "                return \"ç›¸å…³å‚è€ƒæ–‡æ¡£ï¼š\" + parts[1]\n",
    "        return \"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ æ­£åœ¨åŠ è½½æ¯å©´çŸ¥è¯†åº“...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… PDFæ–‡æ¡£åŠ è½½å®Œæˆ: 14é¡µ\n",
      "âœ… Excelæ•°æ®åŠ è½½å®Œæˆ: 111æ¡è®°å½•\n",
      "ğŸ“š å…±åŠ è½½ 125 ä¸ªæ–‡æ¡£\n",
      "ğŸ”ª å¼€å§‹åˆ†å‰²æ–‡æ¡£...\n",
      "âœ… æ–‡æ¡£åˆ†å‰²å®Œæˆ: å…± 131 ä¸ªç‰‡æ®µ\n",
      "ğŸ” æ„å»ºæ™ºèƒ½æ£€ç´¢ç³»ç»Ÿ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1541541/2012295032.py:148: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-large-zh-v1.5\")\n",
      "/mnt/mydisk/zhangxiaohan/anaconda3/envs/py310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ‰ æ™ºèƒ½æ£€ç´¢ç³»ç»Ÿå‡†å¤‡å°±ç»ªï¼\n"
     ]
    }
   ],
   "source": [
    "# 5. ä¸»å‡½æ•°\n",
    "import os\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "# åŠ è½½å¹¶å¤„ç†æ•°æ®\n",
    "print(\"ğŸ”„ æ­£åœ¨åŠ è½½æ¯å©´çŸ¥è¯†åº“...\")\n",
    "try:\n",
    "    raw_docs = load_and_preprocess_data()\n",
    "    if not raw_docs:\n",
    "        print(\"âŒ é”™è¯¯ï¼šæœªåŠ è½½åˆ°ä»»ä½•æ–‡æ¡£æ•°æ®ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶è·¯å¾„å’Œæ ¼å¼\")\n",
    "        \n",
    "    print(f\"ğŸ“š å…±åŠ è½½ {len(raw_docs)} ä¸ªæ–‡æ¡£\")\n",
    "    \n",
    "    # æ–‡æœ¬åˆ†å‰²\n",
    "    print(\"ğŸ”ª å¼€å§‹åˆ†å‰²æ–‡æ¡£...\")\n",
    "    docs_splits = split_documents(raw_docs)\n",
    "    print(f\"âœ… æ–‡æ¡£åˆ†å‰²å®Œæˆ: å…± {len(docs_splits)} ä¸ªç‰‡æ®µ\")\n",
    "    \n",
    "    # åˆ›å»ºå‘é‡å­˜å‚¨\n",
    "    print(\"ğŸ” æ„å»ºæ™ºèƒ½æ£€ç´¢ç³»ç»Ÿ...\")\n",
    "    vector_store = create_vector_store(docs_splits)\n",
    "    print(\"ğŸ‰ æ™ºèƒ½æ£€ç´¢ç³»ç»Ÿå‡†å¤‡å°±ç»ªï¼\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ åˆå§‹åŒ–å¤±è´¥: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ‘¶ äº²çˆ±çš„å‡†å¦ˆå¦ˆï¼Œæ‚¨å¥½å‘¦ï¼æˆ‘æ˜¯æ‚¨çš„æ¯å©´é¡¾é—®å°è´~\n",
      "â¤ï¸ ä»»ä½•å…³äºå­•æœŸæŠ¤ç†ã€æœˆå­ä¸­å¿ƒæˆ–æ–°ç”Ÿå„¿æŠ¤ç†çš„é—®é¢˜éƒ½å¯ä»¥é—®æˆ‘å‘¢ï¼\n",
      "ğŸ“ è¯·è¾“å…¥æ‚¨çš„é—®é¢˜ï¼ˆè¾“å…¥'é€€å‡º'ç»“æŸå’¨è¯¢ï¼‰\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ‘‹ æ„Ÿè°¢å’¨è¯¢ï¼Œç¥æ‚¨å’Œå®å®å¥åº·å¿«ä¹ï¼\n"
     ]
    }
   ],
   "source": [
    "# åˆå§‹åŒ–é¡¾é—®\n",
    "advisor = MaternityAdvisor(vector_store)\n",
    "# ======================\n",
    "# API æ¥å£å®ç°\n",
    "# ======================\n",
    "\n",
    "\n",
    "# äº¤äº’å¾ªç¯\n",
    "print(\"\\nğŸ‘¶ äº²çˆ±çš„å‡†å¦ˆå¦ˆï¼Œæ‚¨å¥½å‘¦ï¼æˆ‘æ˜¯æ‚¨çš„æ¯å©´é¡¾é—®å°è´~\")\n",
    "print(\"â¤ï¸ ä»»ä½•å…³äºå­•æœŸæŠ¤ç†ã€æœˆå­ä¸­å¿ƒæˆ–æ–°ç”Ÿå„¿æŠ¤ç†çš„é—®é¢˜éƒ½å¯ä»¥é—®æˆ‘å‘¢ï¼\")\n",
    "print(\"ğŸ“ è¯·è¾“å…¥æ‚¨çš„é—®é¢˜ï¼ˆè¾“å…¥'é€€å‡º'ç»“æŸå’¨è¯¢ï¼‰\")\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        user_input = input(\"\\nğŸ¤° æ‚¨é—®ï¼š\")\n",
    "        if user_input.lower() in [\"é€€å‡º\", \"exit\"]:\n",
    "            print(\"ğŸ‘‹ æ„Ÿè°¢å’¨è¯¢ï¼Œç¥æ‚¨å’Œå®å®å¥åº·å¿«ä¹ï¼\")\n",
    "            break\n",
    "        \n",
    "        # ç”Ÿæˆå¹¶æ˜¾ç¤ºå›ç­”\n",
    "        response = advisor.generate_response(user_input)\n",
    "        print(\"\\nğŸ’– å°è´ç­”ï¼š\")\n",
    "        print(response)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nğŸ‘‹ å†è§ï¼æœŸå¾…ä¸‹æ¬¡ä¸ºæ‚¨æœåŠ¡~\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ ç³»ç»Ÿé”™è¯¯: {str(e)}\")\n",
    "        print(\"ğŸ”„ è¯·é‡æ–°æé—®æˆ–ç¨åå†è¯•~\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Optional' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 25\u001b[0m\n\u001b[1;32m     16\u001b[0m app\u001b[38;5;241m.\u001b[39madd_middleware(\n\u001b[1;32m     17\u001b[0m     CORSMiddleware,\n\u001b[1;32m     18\u001b[0m     allow_origins\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m     allow_headers\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     22\u001b[0m )\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# å®šä¹‰è¯·æ±‚æ¨¡å‹\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mQuestionRequest\u001b[39;00m(BaseModel):\n\u001b[1;32m     26\u001b[0m     question: \u001b[38;5;28mstr\u001b[39m\n\u001b[1;32m     27\u001b[0m     user_id: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 27\u001b[0m, in \u001b[0;36mQuestionRequest\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mQuestionRequest\u001b[39;00m(BaseModel):\n\u001b[1;32m     26\u001b[0m     question: \u001b[38;5;28mstr\u001b[39m\n\u001b[0;32m---> 27\u001b[0m     user_id: \u001b[43mOptional\u001b[49m[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Optional' is not defined"
     ]
    }
   ],
   "source": [
    "from fastapi import FastAPI, HTTPException, Request\n",
    "from dotenv import load_dotenv\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from pydantic import BaseModel\n",
    "# åˆ›å»ºFastAPIåº”ç”¨\n",
    "advisor = MaternityAdvisor(vector_store)\n",
    "app = FastAPI(\n",
    "    title=\"æ¯å©´æ™ºèƒ½é¡¾é—®API\",\n",
    "    description=\"æä¾›ä¸“ä¸šæ¯å©´å’¨è¯¢æœåŠ¡çš„APIæ¥å£\",\n",
    "    version=\"1.0.0\",\n",
    "    docs_url=\"/docs\",\n",
    "    redoc_url=None\n",
    ")\n",
    "\n",
    "# å…è®¸è·¨åŸŸè¯·æ±‚\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "# å®šä¹‰è¯·æ±‚æ¨¡å‹\n",
    "class QuestionRequest(BaseModel):\n",
    "    question: str\n",
    "    user_id: Optional[str] = None\n",
    "\n",
    "# å®šä¹‰å“åº”æ¨¡å‹\n",
    "class AnswerResponse(BaseModel):\n",
    "    answer: str\n",
    "    related_questions: List[str]\n",
    "\n",
    "# APIç«¯ç‚¹\n",
    "@app.post(\"/ask\", response_model=AnswerResponse, summary=\"å‘æ¯å©´é¡¾é—®æé—®\")\n",
    "async def ask_question(request: QuestionRequest):\n",
    "    \"\"\"\n",
    "    å‘æ¯å©´æ™ºèƒ½é¡¾é—®æé—®å¹¶è·å–å›ç­”\n",
    "    \n",
    "    - **question**: æ‚¨è¦å’¨è¯¢çš„é—®é¢˜\n",
    "    - **user_id**: å¯é€‰ç”¨æˆ·IDï¼Œç”¨äºä¸ªæ€§åŒ–æœåŠ¡\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # è°ƒç”¨é¡¾é—®ç³»ç»Ÿè·å–å›ç­”\n",
    "        result = advisor.ask_question(request.question)\n",
    "        return AnswerResponse(\n",
    "            answer=result[\"answer\"],\n",
    "            related_questions=result[\"related_questions\"]\n",
    "        )\n",
    "    except Exception as e:\n",
    "        # è®°å½•è¯¦ç»†é”™è¯¯æ—¥å¿—\n",
    "        print(f\"APIè¯·æ±‚å¤„ç†é”™è¯¯: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        raise HTTPException(status_code=500, detail=\"æœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åå†è¯•\")\n",
    "\n",
    "# å¥åº·æ£€æŸ¥ç«¯ç‚¹\n",
    "@app.get(\"/health\", summary=\"æœåŠ¡å¥åº·æ£€æŸ¥\")\n",
    "async def health_check():\n",
    "    \"\"\"æ£€æŸ¥APIæœåŠ¡æ˜¯å¦æ­£å¸¸è¿è¡Œ\"\"\"\n",
    "    return {\n",
    "        \"status\": \"healthy\",\n",
    "        \"service\": \"maternity-advisor\",\n",
    "        \"version\": \"1.0.0\",\n",
    "        \"timestamp\": time.time()\n",
    "    }\n",
    "\n",
    "# æœåŠ¡ä¿¡æ¯ç«¯ç‚¹\n",
    "@app.get(\"/info\", summary=\"æœåŠ¡ä¿¡æ¯\")\n",
    "async def service_info():\n",
    "    \"\"\"è·å–æœåŠ¡ä¿¡æ¯\"\"\"\n",
    "    return {\n",
    "        \"service\": \"æ¯å©´æ™ºèƒ½é¡¾é—®API\",\n",
    "        \"version\": \"1.0.0\",\n",
    "        \"model\": \"è±†åŒ…1.5-128k\",\n",
    "        \"knowledge_base\": {\n",
    "            \"pdf\": PDF_PATH,\n",
    "            \"excel\": EXCEL_PATH,\n",
    "            \"documents\": len(raw_docs),\n",
    "            \"chunks\": len(docs_splits)\n",
    "        }\n",
    "    }\n",
    "\n",
    "# è¿è¡ŒæœåŠ¡\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
